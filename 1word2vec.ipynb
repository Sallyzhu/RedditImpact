{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4n9TsbGw-f"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:43.846679Z",
     "iopub.status.busy": "2022-12-14T06:16:43.846415Z",
     "iopub.status.idle": "2022-12-14T06:16:43.850587Z",
     "shell.execute_reply": "2022-12-14T06:16:43.850017Z"
    },
    "id": "0nbI5DtDGw-i"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word2vec\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word2vec.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haJUNjSB60Kh"
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99d4ky2lWFvn"
   },
   "source": [
    "word2vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through word2vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
    "\n",
    "Note: This tutorial is based on [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf) and [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It is not an exact implementation of the papers. Rather, it is intended to illustrate the key ideas.\n",
    "\n",
    "These papers proposed two methods for learning representations of words:\n",
    "\n",
    "*   **Continuous bag-of-words model**: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "*   **Continuous skip-gram model**: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
    "\n",
    "You'll use the skip-gram approach in this tutorial. First, you'll explore skip-grams and other concepts using a single sentence for illustration. Next, you'll train your own word2vec model on a small dataset. This tutorial also contains code to export the trained embeddings and visualize them in the [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP00WlaMWBZC"
   },
   "source": [
    "## Skip-gram and negative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr2wjv0bW236"
   },
   "source": [
    "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of `target_word`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICjc-McbaVTd"
   },
   "source": [
    "Consider the following sentence of eight words:\n",
    "\n",
    "> The wide road shimmered in the hot sun.\n",
    "\n",
    "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered a `context word`. Below is a table of skip-grams for target words based on different window sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKE87IKT_YT8"
   },
   "source": [
    "Note: For this tutorial, a window size of `n` implies n words on each side with a total window span of 2*n+1 words across a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsCwQ07E8mqU"
   },
   "source": [
    "![word2vec_skipgrams](https://tensorflow.org/tutorials/text/images/word2vec_skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK1gN1jwkMpU"
   },
   "source": [
    "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words *w<sub>1</sub>, w<sub>2</sub>, ... w<sub>T</sub>*, the objective can be written as the average log probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pILO_iAc84e-"
   },
   "source": [
    "![word2vec_skipgram_objective](https://tensorflow.org/tutorials/text/images/word2vec_skipgram_objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsy6TUbtnz_K"
   },
   "source": [
    "where `c` is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P81Qavbb9APd"
   },
   "source": [
    "![word2vec_full_softmax](https://tensorflow.org/tutorials/text/images/word2vec_full_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axZvd-hhotVB"
   },
   "source": [
    "where *v* and *v<sup>'<sup>* are target and context vector representations of words and *W* is vocabulary size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoLzxbqSpT6_"
   },
   "source": [
    "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large (10<sup>5</sup>-10<sup>7</sup>) terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5VWYtmFzHkU"
   },
   "source": [
    "The [noise contrastive estimation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) (NCE) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modeling the word distribution, the NCE loss can be [simplified](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to use negative sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTZBPf1RsOsg"
   },
   "source": [
    "The simplified negative sampling objective for a target word is to distinguish  the context word from `num_ns` negative samples drawn from noise distribution *P<sub>n</sub>(w)* of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and `num_ns` negative samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl0rSfHjt6Mf"
   },
   "source": [
    "A negative sample is defined as a `(target_word, context_word)` pair such that the `context_word` does not appear in the `window_size` neighborhood of the `target_word`. For the example sentence, these are a few potential negative samples (when `window_size` is `2`).\n",
    "\n",
    "```\n",
    "(hot, shimmered)\n",
    "(wide, hot)\n",
    "(wide, sun)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kq0q2uqbucFg"
   },
   "source": [
    "In the next section, you'll generate skip-grams and negative samples for a single sentence. You'll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4-Hpe1CH16"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:43.855578Z",
     "iopub.status.busy": "2022-12-14T06:16:43.855106Z",
     "iopub.status.idle": "2022-12-14T06:16:45.871623Z",
     "shell.execute_reply": "2022-12-14T06:16:45.870903Z"
    },
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.875942Z",
     "iopub.status.busy": "2022-12-14T06:16:45.875148Z",
     "iopub.status.idle": "2022-12-14T06:16:45.880371Z",
     "shell.execute_reply": "2022-12-14T06:16:45.879788Z"
    },
    "id": "10pyUMFkGKVQ"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.883958Z",
     "iopub.status.busy": "2022-12-14T06:16:45.883417Z",
     "iopub.status.idle": "2022-12-14T06:16:45.886553Z",
     "shell.execute_reply": "2022-12-14T06:16:45.885943Z"
    },
    "id": "XkJ5299Tek6B"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW-g5buCHwh3"
   },
   "source": [
    "### Vectorize an example sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8TfZIgoQrcP"
   },
   "source": [
    "Consider the following sentence:\n",
    "\n",
    "> The wide road shimmered in the hot sun.\n",
    "\n",
    "Tokenize the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.889992Z",
     "iopub.status.busy": "2022-12-14T06:16:45.889435Z",
     "iopub.status.idle": "2022-12-14T06:16:45.892924Z",
     "shell.execute_reply": "2022-12-14T06:16:45.892367Z"
    },
    "id": "bsl7jBzV6_KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU-bs1XtThEw"
   },
   "source": [
    "Create a vocabulary to save mappings from tokens to integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.896343Z",
     "iopub.status.busy": "2022-12-14T06:16:45.895753Z",
     "iopub.status.idle": "2022-12-14T06:16:45.899791Z",
     "shell.execute_reply": "2022-12-14T06:16:45.899249Z"
    },
    "id": "UdYv1HJUQ8XA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpuP43Dddasr"
   },
   "source": [
    "Create an inverse vocabulary to save mappings from integer indices to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.903175Z",
     "iopub.status.busy": "2022-12-14T06:16:45.902545Z",
     "iopub.status.idle": "2022-12-14T06:16:45.906261Z",
     "shell.execute_reply": "2022-12-14T06:16:45.905621Z"
    },
    "id": "o9ULAJYtEvKl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3qtuyxIRyii"
   },
   "source": [
    "Vectorize your sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.909437Z",
     "iopub.status.busy": "2022-12-14T06:16:45.908995Z",
     "iopub.status.idle": "2022-12-14T06:16:45.912545Z",
     "shell.execute_reply": "2022-12-14T06:16:45.911950Z"
    },
    "id": "CsB3-9uQQYyl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox1I28JRIOdM"
   },
   "source": [
    "### Generate skip-grams from one sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7NNKAmSiHvy"
   },
   "source": [
    "The `tf.keras.preprocessing.sequence` module provides useful functions that simplify data preparation for word2vec. You can use the `tf.keras.preprocessing.sequence.skipgrams` to generate skip-gram pairs from the `example_sequence` with a given `window_size` from tokens in the range `[0, vocab_size)`.\n",
    "\n",
    "Note: `negative_samples` is set to `0` here, as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.915818Z",
     "iopub.status.busy": "2022-12-14T06:16:45.915567Z",
     "iopub.status.idle": "2022-12-14T06:16:45.919543Z",
     "shell.execute_reply": "2022-12-14T06:16:45.919010Z"
    },
    "id": "USAJxW4RD7pn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc9uhiMwY-AQ"
   },
   "source": [
    "Print a few positive skip-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.922721Z",
     "iopub.status.busy": "2022-12-14T06:16:45.922210Z",
     "iopub.status.idle": "2022-12-14T06:16:45.925741Z",
     "shell.execute_reply": "2022-12-14T06:16:45.925130Z"
    },
    "id": "SCnqEukIE9pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3): (shimmered, road)\n",
      "(2, 1): (wide, the)\n",
      "(5, 1): (in, the)\n",
      "(1, 7): (the, sun)\n",
      "(1, 3): (the, road)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ua9PkMTISF0"
   },
   "source": [
    "### Negative sampling for one skip-gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esqn8WBfZnEK"
   },
   "source": [
    "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgH3aSvw3xTD"
   },
   "source": [
    "Key point: `num_ns` (the number of negative samples per a positive context word) in the `[5, 20]` range is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while `num_ns` in the `[2, 5]` range suffices for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:45.929229Z",
     "iopub.status.busy": "2022-12-14T06:16:45.928695Z",
     "iopub.status.idle": "2022-12-14T06:16:49.351345Z",
     "shell.execute_reply": "2022-12-14T06:16:49.350683Z"
    },
    "id": "m_LmdzqIGr5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
      "['wide', 'the', 'shimmered', 'road']\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MSxWCrLIalp"
   },
   "source": [
    "### Construct one training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6uEWdj8vKKv"
   },
   "source": [
    "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labeled as `1`) and negative samples (labeled as `0`) for each target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.354403Z",
     "iopub.status.busy": "2022-12-14T06:16:49.354154Z",
     "iopub.status.idle": "2022-12-14T06:16:49.360555Z",
     "shell.execute_reply": "2022-12-14T06:16:49.359943Z"
    },
    "id": "zSiZwifuLvHf"
   },
   "outputs": [],
   "source": [
    "# Reduce a dimension so you can use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIJeoFCAwtXJ"
   },
   "source": [
    "Check out the context and the corresponding labels for the target word from the skip-gram example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.363871Z",
     "iopub.status.busy": "2022-12-14T06:16:49.363377Z",
     "iopub.status.idle": "2022-12-14T06:16:49.370292Z",
     "shell.execute_reply": "2022-12-14T06:16:49.369724Z"
    },
    "id": "tzyCPCuZwmdL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 4\n",
      "target_word     : shimmered\n",
      "context_indices : [3 2 1 4 3]\n",
      "context_words   : ['road', 'wide', 'the', 'shimmered', 'road']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBtTcUVQr8EO"
   },
   "source": [
    "A tuple of `(target, context, label)` tensors constitutes one training example for training your skip-gram negative sampling word2vec model. Notice that the target is of shape `(1,)` while the context and label are of shape `(1+num_ns,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.373252Z",
     "iopub.status.busy": "2022-12-14T06:16:49.372749Z",
     "iopub.status.idle": "2022-12-14T06:16:49.376531Z",
     "shell.execute_reply": "2022-12-14T06:16:49.375906Z"
    },
    "id": "x-FwkR8jx9-Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : 4\n",
      "context : tf.Tensor([3 2 1 4 3], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bRJIlow4Dlv"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWkuha0oykG5"
   },
   "source": [
    "This diagram summarizes the procedure of generating a training example from a sentence:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KlwdiAa9crJ"
   },
   "source": [
    "![word2vec_negative_sampling](https://tensorflow.org/tutorials/text/images/word2vec_negative_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37e53f07f67c"
   },
   "source": [
    "Notice that the words `temperature` and `code` are not part of the input sentence. They belong to the vocabulary like certain other indices used in the diagram above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wmdO_MEIpaM"
   },
   "source": [
    "## Compile all steps into one function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLKwNAczHsKg"
   },
   "source": [
    "### Skip-gram sampling table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUUK3uDtFNFE"
   },
   "source": [
    "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as `the`, `is`, `on`) don't add much useful information  for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPtbv7zNP7Dx"
   },
   "source": [
    "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to  generate a word-frequency rank based probabilistic sampling table and pass it to the `skipgrams` function. Inspect the sampling probabilities for a `vocab_size` of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.380087Z",
     "iopub.status.busy": "2022-12-14T06:16:49.379646Z",
     "iopub.status.idle": "2022-12-14T06:16:49.383418Z",
     "shell.execute_reply": "2022-12-14T06:16:49.382867Z"
    },
    "id": "Rn9zAnDccyRg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHvSptcPk5fp"
   },
   "source": [
    "`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRHMssMmHgH-"
   },
   "source": [
    "Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj--8RFK6fgW"
   },
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy5hl4lQ0B2M"
   },
   "source": [
    "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.386741Z",
     "iopub.status.busy": "2022-12-14T06:16:49.386489Z",
     "iopub.status.idle": "2022-12-14T06:16:49.392328Z",
     "shell.execute_reply": "2022-12-14T06:16:49.391784Z"
    },
    "id": "63INISDEX1Hu"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shvPC8Ji2cMK"
   },
   "source": [
    "## Prepare training data for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5mbZsZu6uKg"
   },
   "source": [
    "With an understanding of how to work with one sentence for a skip-gram negative sampling based word2vec model, you can proceed to generate training examples from a larger list of sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFlikI6L26nh"
   },
   "source": [
    "### Download text corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEFavOgN98al"
   },
   "source": [
    "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.395634Z",
     "iopub.status.busy": "2022-12-14T06:16:49.395110Z",
     "iopub.status.idle": "2022-12-14T06:16:49.478267Z",
     "shell.execute_reply": "2022-12-14T06:16:49.477618Z"
    },
    "id": "QFkitxzVVaAi"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"DepAnx1922_selftext.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOsbLq8a37dr"
   },
   "source": [
    "Read the text from the file and print the first few lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.481773Z",
     "iopub.status.busy": "2022-12-14T06:16:49.481204Z",
     "iopub.status.idle": "2022-12-14T06:16:49.490712Z",
     "shell.execute_reply": "2022-12-14T06:16:49.490067Z"
    },
    "id": "lfgnsUw3ofMD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "god get people sometimes people tendency make upset angry straight accept alone good thing people try friends people ask questions show genuine interest wanting friends answer reciprocate back guess text due fact brings attention probably even care going meor old friend refuses acknowledge well fuck toomods cuss ill edit roommate gone somewhat friends habiting honestly gone numb high school numbed point part mind cares wants friends part care says try people give shit get ask boring repetitive questions mostall time texting know less purpose tell dayi could form specific questions showing genuine interest always prefered face face conversation weather person video call allows see react certain things keep conversation interesting see texting monotone guess old school like honestly stopped caring decides friends used aloneness solitude bother much used still started think may something wrong pushes people away think one problems isproblem people everyone else age albeit vaping smoking weed partying etc stopped caring try find reached point say accept stand dodont screw changing people really get nerves sometimes thank reading rant needed vent\n",
      "first post relatively new reddit two years every day wake first thought comes want commit suicide problems pain anything problem death every day get college back without speaking anyone lock bedroom cry cut never anyone support friends moved away parents know handle sometimes want something happen kill wanted someone side since honestly know kill pain\n",
      "apologies allowed feel free point elsewhere friend going really hard time right like know help know hes new medication willing counselor psychologist tried lot want know help\n",
      "know happened issues thought got ugh\n",
      "successfully pushed anyone ever mattered created sorts scenarios head right thing sit one left starting regret ever done carry emotionless blank face around smile talk people people weirded suspect afraid angry person friendly guy horrible anxiety intrusive thoughts make act certain way reading still people around care hold hard find thanks anyone read far\n",
      "really drunk night guess told people going kill selftheyre beyond worried one stopped talking issues figure friends guess darkest secret came sure really wanted drunk guess really wanted confused\n",
      "struggling seems like forever therapist knows keep unpacking every weekly session almost running joke often mention dirty dishes therapy sitting months growing things stinking place throwing takeout boxes trash top physical manifestation inner state function sort work fulltime school parttime socialize exhausting making apartment nice space feels good hard finally four months something clicked unintentionally maybe sliver resolve hidden muck came really know maybe nose finally fed smelling crap called work friday could back bed seriously could face slept six hours woke started cleaning actually surprised lay couch perpetual ugh state slow tedious feels good walk door feel less anxiety space space know last hoping help simplify life least look physical mess dealing mental mess\n",
      "still says loves want feel like absolute shit hopefully guys cheer little bit feeling really lonely\n",
      "used seasonal dips depression pick manic recovery ampxb feel different feels solid hurts somewhere else usually verge tears used feeling hopeless may like mourning ampxb good relationship could get hump right direction lots conversations ultimately calm good bye\n",
      "feel like boring appearance every day like anything different good bad ill attract attention want used one home would love paint draw read play music cook etc roommates got home would quickly stop whatever pretend studying watching even energy anything alone issue family thought feel way roommates well want attention people feel exhausted time even deep conversations never fully say feel think much much focus anyone else feel way\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file,'r', encoding=\"utf-8\") as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:10]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTNZYqUs5C2V"
   },
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.494127Z",
     "iopub.status.busy": "2022-12-14T06:16:49.493689Z",
     "iopub.status.idle": "2022-12-14T06:16:49.532024Z",
     "shell.execute_reply": "2022-12-14T06:16:49.531430Z"
    },
    "id": "ViDrwy-HjAs9"
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfsc88zE9upk"
   },
   "source": [
    "### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfgZo8zR94KK"
   },
   "source": [
    "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.535643Z",
     "iopub.status.busy": "2022-12-14T06:16:49.535050Z",
     "iopub.status.idle": "2022-12-14T06:16:49.549393Z",
     "shell.execute_reply": "2022-12-14T06:16:49.548584Z"
    },
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g92LuvnyBmz1"
   },
   "source": [
    "Call `TextVectorization.adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:49.552886Z",
     "iopub.status.busy": "2022-12-14T06:16:49.552419Z",
     "iopub.status.idle": "2022-12-14T06:16:50.799157Z",
     "shell.execute_reply": "2022-12-14T06:16:50.798400Z"
    },
    "id": "seZau_iYMPFT"
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg2z7eeHMnH-"
   },
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `TextVectorization.get_vocabulary`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:50.803496Z",
     "iopub.status.busy": "2022-12-14T06:16:50.802880Z",
     "iopub.status.idle": "2022-12-14T06:16:50.812989Z",
     "shell.execute_reply": "2022-12-14T06:16:50.812408Z"
    },
    "id": "jgw9pTA7MRaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'like', 'feel', 'know', 'get', 'want', 'life', 'even', 'time', 'really', 'would', 'anxiety', 'people', 'one', 'going', 'think', 'day', 'much', 'things']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQ30Tx6KA2G"
   },
   "source": [
    "The `vectorize_layer` can now be used to generate vectors for each element in the `text_ds` (a `tf.data.Dataset`). Apply `Dataset.batch`, `Dataset.prefetch`, `Dataset.map`, and `Dataset.unbatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:50.816540Z",
     "iopub.status.busy": "2022-12-14T06:16:50.815833Z",
     "iopub.status.idle": "2022-12-14T06:16:50.859037Z",
     "shell.execute_reply": "2022-12-14T06:16:50.858471Z"
    },
    "id": "yUVYrDp0araQ"
   },
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YyH_SYzB72p"
   },
   "source": [
    "### Obtain sequences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUQLX0_KaRC"
   },
   "source": [
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a word2vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples.\n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TensorFlow Python/NumPy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:50.862609Z",
     "iopub.status.busy": "2022-12-14T06:16:50.862144Z",
     "iopub.status.idle": "2022-12-14T06:16:55.272731Z",
     "shell.execute_reply": "2022-12-14T06:16:55.271988Z"
    },
    "id": "sGXoOh9y11pM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652452\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDc4riukLTqg"
   },
   "source": [
    "Inspect a few examples from `sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:55.276912Z",
     "iopub.status.busy": "2022-12-14T06:16:55.276162Z",
     "iopub.status.idle": "2022-12-14T06:16:55.280788Z",
     "shell.execute_reply": "2022-12-14T06:16:55.280147Z"
    },
    "id": "WZf1RIbB2Dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 436    5   13  107   13 3869   36  487  396  669] => ['god', 'get', 'people', 'sometimes', 'people', 'tendency', 'make', 'upset', 'angry', 'straight']\n",
      "[  72  204 1651  112  511  137   24   41   17  261] => ['first', 'post', 'relatively', 'new', 'reddit', 'two', 'years', 'every', 'day', 'wake']\n",
      "[3025  976    3  481   80 4057  110   15   10   69] => ['apologies', 'allowed', 'feel', 'free', 'point', 'elsewhere', 'friend', 'going', 'really', 'hard']\n",
      "[   4  217  207   75   34 2080    0    0    0    0] => ['know', 'happened', 'issues', 'thought', 'got', 'ugh', '', '', '', '']\n",
      "[3223 1086   33   62 3972 1774 2356 2033  147   55] => ['successfully', 'pushed', 'anyone', 'ever', 'mattered', 'created', 'sorts', 'scenarios', 'head', 'right']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzSOjNwCWNh"
   },
   "source": [
    "### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BehvYr-nEKyY"
   },
   "source": [
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:16:55.284370Z",
     "iopub.status.busy": "2022-12-14T06:16:55.283912Z",
     "iopub.status.idle": "2022-12-14T06:17:47.545102Z",
     "shell.execute_reply": "2022-12-14T06:17:47.544351Z"
    },
    "id": "44DJ22M6nX5o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652452/652452 [05:24<00:00, 2009.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (2849652,)\n",
      "contexts.shape: (2849652, 5)\n",
      "labels.shape: (2849652, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PqsusOFEpc"
   },
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnFVySViQTj"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.548742Z",
     "iopub.status.busy": "2022-12-14T06:17:47.548461Z",
     "iopub.status.idle": "2022-12-14T06:17:47.564750Z",
     "shell.execute_reply": "2022-12-14T06:17:47.564070Z"
    },
    "id": "nbu8PxPSnVY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyrNX6Fs6K3F"
   },
   "source": [
    "Apply `Dataset.cache` and `Dataset.prefetch` to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.568167Z",
     "iopub.status.busy": "2022-12-14T06:17:47.567563Z",
     "iopub.status.idle": "2022-12-14T06:17:47.574488Z",
     "shell.execute_reply": "2022-12-14T06:17:47.573873Z"
    },
    "id": "Y5Ueg6bcFPVL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-CmUMszyEf"
   },
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQFqaBMPwBqC"
   },
   "source": [
    "The word2vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc7kTbiwD9sy"
   },
   "source": [
    "### Subclassed word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvr9pM1G1sQN"
   },
   "source": [
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your word2vec model with the following layers:\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAwuIqqw7-7"
   },
   "source": [
    "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.578098Z",
     "iopub.status.busy": "2022-12-14T06:17:47.577566Z",
     "iopub.status.idle": "2022-12-14T06:17:47.582833Z",
     "shell.execute_reply": "2022-12-14T06:17:47.582217Z"
    },
    "id": "i9ec-sS6xd8Z"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLKz9LFECXu"
   },
   "source": [
    "### Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Md-9QanqBM"
   },
   "source": [
    "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
    "\n",
    "``` python\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "```\n",
    "\n",
    "It's time to build your model! Instantiate your word2vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.585954Z",
     "iopub.status.busy": "2022-12-14T06:17:47.585419Z",
     "iopub.status.idle": "2022-12-14T06:17:47.605196Z",
     "shell.execute_reply": "2022-12-14T06:17:47.604558Z"
    },
    "id": "ekQg_KbWnnmQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3MUMrluqNX2"
   },
   "source": [
    "Also define a callback to log training statistics for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.608567Z",
     "iopub.status.busy": "2022-12-14T06:17:47.607992Z",
     "iopub.status.idle": "2022-12-14T06:17:47.611985Z",
     "shell.execute_reply": "2022-12-14T06:17:47.611262Z"
    },
    "id": "9d-ftBCeEZIR"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wEBotlGZ7B"
   },
   "source": [
    "Train the model on the `dataset` for some number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:47.615139Z",
     "iopub.status.busy": "2022-12-14T06:17:47.614549Z",
     "iopub.status.idle": "2022-12-14T06:17:59.161446Z",
     "shell.execute_reply": "2022-12-14T06:17:59.160714Z"
    },
    "id": "gmC1BJalEZIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2782/2782 [==============================] - 10s 4ms/step - loss: 1.3284 - accuracy: 0.4360\n",
      "Epoch 2/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.1992 - accuracy: 0.5100\n",
      "Epoch 3/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.1456 - accuracy: 0.5373\n",
      "Epoch 4/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.1088 - accuracy: 0.5555\n",
      "Epoch 5/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.0802 - accuracy: 0.5693\n",
      "Epoch 6/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.0570 - accuracy: 0.5802\n",
      "Epoch 7/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.0379 - accuracy: 0.5890\n",
      "Epoch 8/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.0221 - accuracy: 0.5962\n",
      "Epoch 9/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 1.0088 - accuracy: 0.6021\n",
      "Epoch 10/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9975 - accuracy: 0.6072\n",
      "Epoch 11/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9878 - accuracy: 0.6114\n",
      "Epoch 12/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9794 - accuracy: 0.6150\n",
      "Epoch 13/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9719 - accuracy: 0.6183\n",
      "Epoch 14/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9653 - accuracy: 0.6211\n",
      "Epoch 15/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9594 - accuracy: 0.6237\n",
      "Epoch 16/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9540 - accuracy: 0.6260\n",
      "Epoch 17/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9490 - accuracy: 0.6280\n",
      "Epoch 18/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9446 - accuracy: 0.6300\n",
      "Epoch 19/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9404 - accuracy: 0.6317\n",
      "Epoch 20/20\n",
      "2782/2782 [==============================] - 9s 3ms/step - loss: 0.9366 - accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed04080be0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wze38jG57XvZ"
   },
   "source": [
    "TensorBoard now shows the word2vec model's accuracy and loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awF3iRQCZOLj"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/word2vec_tensorboard.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaDW2tIIz8fL"
   },
   "source": [
    "## Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:59.165514Z",
     "iopub.status.busy": "2022-12-14T06:17:59.164817Z",
     "iopub.status.idle": "2022-12-14T06:17:59.177593Z",
     "shell.execute_reply": "2022-12-14T06:17:59.176883Z"
    },
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWzdmUzS8Sl4"
   },
   "source": [
    "Create and save the vectors and metadata files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:59.180595Z",
     "iopub.status.busy": "2022-12-14T06:17:59.180376Z",
     "iopub.status.idle": "2022-12-14T06:17:59.499643Z",
     "shell.execute_reply": "2022-12-14T06:17:59.498985Z"
    },
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T8KcThhIU8-"
   },
   "source": [
    "Download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:17:59.503885Z",
     "iopub.status.busy": "2022-12-14T06:17:59.503255Z",
     "iopub.status.idle": "2022-12-14T06:17:59.507178Z",
     "shell.execute_reply": "2022-12-14T06:17:59.506344Z"
    },
    "id": "lUsjQOKMIV2z"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the word vectors and metadata to files\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word vectors and metadata to files\n",
    "word_vectors = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "metadata = list(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(vocab):\n",
    "    if index == 0: continue  # Skip the padding token\n",
    "    vec = weights[index]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS_uMeMw3Xpj"
   },
   "source": [
    "## Next steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find similar words for a single input word\n",
    "def find_similar_words(word, topn=50):\n",
    "    index = vocab.index(word)\n",
    "    vec = weights[index]\n",
    "    similarities = np.dot(weights, vec)\n",
    "    indices = similarities.argsort()[-topn:][::-1]\n",
    "    similar_words = [vocab[i] for i in indices]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find similar words for two input words\n",
    "def find_similar_words_2(word1, word2, topn=50):\n",
    "    index1 = vocab.index(word1)\n",
    "    index2 = vocab.index(word2)\n",
    "    vec = weights[index1] + weights[index2]\n",
    "    similarities = np.dot(weights, vec)\n",
    "    indices = similarities.argsort()[-topn:][::-1]\n",
    "    similar_words = [vocab[i] for i in indices]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unemployment',\n",
       " 'income',\n",
       " 'rent',\n",
       " 'jobless',\n",
       " 'credit',\n",
       " 'employment',\n",
       " 'companies',\n",
       " 'fortunately',\n",
       " 'laid',\n",
       " 'apply',\n",
       " 'positions',\n",
       " 'dollars',\n",
       " 'contract',\n",
       " 'employees',\n",
       " 'parttime',\n",
       " 'salary',\n",
       " 'staff',\n",
       " 'report',\n",
       " 'paycheck',\n",
       " 'programs',\n",
       " 'graduating',\n",
       " 'luckily',\n",
       " 'pharmacy',\n",
       " 'parking',\n",
       " 'bills',\n",
       " 'pays',\n",
       " 'california',\n",
       " 'strength',\n",
       " 'homeless',\n",
       " 'encouraged',\n",
       " 'tasks',\n",
       " 'patients',\n",
       " 'promises',\n",
       " 'traumatized',\n",
       " 'hometown',\n",
       " 'wage',\n",
       " 'adjust',\n",
       " 'car',\n",
       " 'fulltime',\n",
       " 'circumstances',\n",
       " 'tech',\n",
       " 'pocket',\n",
       " 'demanding',\n",
       " 'repair',\n",
       " 'surprised',\n",
       " 'temporarily',\n",
       " 'courses',\n",
       " 'transition',\n",
       " 'hired',\n",
       " 'admitting']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words('unemployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lockdown',\n",
       " 'shelter',\n",
       " 'loans',\n",
       " 'jobless',\n",
       " 'significantly',\n",
       " 'familys',\n",
       " 'quarantine',\n",
       " 'grandfather',\n",
       " 'holidays',\n",
       " 'pandemic',\n",
       " 'adjust',\n",
       " 'seizure',\n",
       " 'employment',\n",
       " 'appetite',\n",
       " 'staying',\n",
       " 'foster',\n",
       " 'coronavirus',\n",
       " 'roommates',\n",
       " 'dreading',\n",
       " 'distancing',\n",
       " 'graduation',\n",
       " 'decline',\n",
       " 'holiday',\n",
       " 'toll',\n",
       " 'city',\n",
       " 'isolation',\n",
       " 'political',\n",
       " 'proof',\n",
       " 'grasp',\n",
       " 'unemployment',\n",
       " 'wednesday',\n",
       " 'summer',\n",
       " 'corona',\n",
       " 'notices',\n",
       " 'graduating',\n",
       " 'fathers',\n",
       " 'drastically',\n",
       " 'ringing',\n",
       " 'horrific',\n",
       " 'acid',\n",
       " 'august',\n",
       " 'reminds',\n",
       " 'storm',\n",
       " 'events',\n",
       " 'salary',\n",
       " 'media',\n",
       " 'challenges',\n",
       " 'cousins',\n",
       " 'arrived',\n",
       " 'attended']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words('lockdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance',\n",
       " 'use',\n",
       " 'green',\n",
       " 'recommendations',\n",
       " 'magnesium',\n",
       " 'challenges',\n",
       " 'smoking',\n",
       " 'ketamine',\n",
       " 'savings',\n",
       " 'abuse',\n",
       " 'tea',\n",
       " 'programs',\n",
       " 'characters',\n",
       " 'marijuana',\n",
       " 'dismissed',\n",
       " 'sheer',\n",
       " 'occasional',\n",
       " 'warning',\n",
       " 'methods',\n",
       " 'abilify',\n",
       " 'mindfulness',\n",
       " 'thesis',\n",
       " 'kiss',\n",
       " 'rehab',\n",
       " 'neglect',\n",
       " 'illegal',\n",
       " 'army',\n",
       " 'tool',\n",
       " 'poverty',\n",
       " 'heat',\n",
       " 'chemicals',\n",
       " 'aka',\n",
       " 'alcohol',\n",
       " 'addition',\n",
       " 'foster',\n",
       " 'vitamins',\n",
       " 'nicotine',\n",
       " 'mechanism',\n",
       " 'script',\n",
       " 'porn',\n",
       " 'useful',\n",
       " 'acceptance',\n",
       " 'sends',\n",
       " 'reflection',\n",
       " 'longterm',\n",
       " 'compassion',\n",
       " 'healthier',\n",
       " 'structure',\n",
       " 'label',\n",
       " 'relieve']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words_2('substance', 'use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from difflib import get_close_matches\n",
    "# Define a function to find similar words for a single input word\n",
    "def find_CLOSEsimilar_words(word, topn=50):\n",
    "    matches = get_close_matches(word, vocab, n=1, cutoff=0.8)\n",
    "    if len(matches) > 0:\n",
    "        index = vocab.index(matches[0])\n",
    "        vec = weights[index]\n",
    "        similarities = np.dot(weights, vec)\n",
    "        indices = similarities.argsort()[-topn:][::-1]\n",
    "        similar_words = [vocab[i] for i in indices]\n",
    "    else:\n",
    "        similar_words = []\n",
    "    return similar_words\n",
    "# Define a function to find similar words for two input words\n",
    "def find_CLOSEsimilar_words_2(word1, word2, topn=50):\n",
    "    matches1 = get_close_matches(word1, vocab, n=1, cutoff=0.8)\n",
    "    matches2 = get_close_matches(word2, vocab, n=1, cutoff=0.8)\n",
    "    if len(matches1) > 0 and len(matches2) > 0:\n",
    "        index1 = vocab.index(matches1[0])\n",
    "        index2 = vocab.index(matches2[0])\n",
    "        vec = weights[index1] + weights[index2]\n",
    "        similarities = np.dot(weights, vec)\n",
    "        indices = similarities.argsort()[-topn:][::-1]\n",
    "        similar_words = [vocab[i] for i in indices]\n",
    "    else:\n",
    "        similar_words = []\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance',\n",
       " 'rehab',\n",
       " 'army',\n",
       " 'occasional',\n",
       " 'pregnancy',\n",
       " 'tool',\n",
       " 'smoking',\n",
       " 'savings',\n",
       " 'foster',\n",
       " 'poverty',\n",
       " 'abuse',\n",
       " 'characters',\n",
       " 'addition',\n",
       " 'underlying',\n",
       " 'income',\n",
       " 'porn',\n",
       " 'verbal',\n",
       " 'magnesium',\n",
       " 'sheer',\n",
       " 'hopelessness',\n",
       " 'dismissed',\n",
       " 'distress',\n",
       " 'military',\n",
       " 'programs',\n",
       " 'thesis',\n",
       " 'loans',\n",
       " 'sugar',\n",
       " 'heat',\n",
       " 'warning',\n",
       " 'illegal',\n",
       " 'impacted',\n",
       " 'healthier',\n",
       " 'significantly',\n",
       " 'neglect',\n",
       " 'ideation',\n",
       " 'challenges',\n",
       " 'green',\n",
       " 'alcohol',\n",
       " 'compassion',\n",
       " 'abilify',\n",
       " 'recommendations',\n",
       " 'marijuana',\n",
       " 'gender',\n",
       " 'sexual',\n",
       " 'partially',\n",
       " 'addict',\n",
       " 'housing',\n",
       " 'term',\n",
       " 'fulfilling',\n",
       " 'reads']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('substance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance',\n",
       " 'use',\n",
       " 'green',\n",
       " 'recommendations',\n",
       " 'magnesium',\n",
       " 'challenges',\n",
       " 'smoking',\n",
       " 'ketamine',\n",
       " 'savings',\n",
       " 'abuse',\n",
       " 'tea',\n",
       " 'programs',\n",
       " 'characters',\n",
       " 'marijuana',\n",
       " 'dismissed',\n",
       " 'sheer',\n",
       " 'occasional',\n",
       " 'warning',\n",
       " 'methods',\n",
       " 'abilify',\n",
       " 'mindfulness',\n",
       " 'thesis',\n",
       " 'kiss',\n",
       " 'rehab',\n",
       " 'neglect',\n",
       " 'illegal',\n",
       " 'army',\n",
       " 'tool',\n",
       " 'poverty',\n",
       " 'heat',\n",
       " 'chemicals',\n",
       " 'aka',\n",
       " 'alcohol',\n",
       " 'addition',\n",
       " 'foster',\n",
       " 'vitamins',\n",
       " 'nicotine',\n",
       " 'mechanism',\n",
       " 'script',\n",
       " 'porn',\n",
       " 'useful',\n",
       " 'acceptance',\n",
       " 'sends',\n",
       " 'reflection',\n",
       " 'longterm',\n",
       " 'compassion',\n",
       " 'healthier',\n",
       " 'structure',\n",
       " 'label',\n",
       " 'relieve']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words_2('substance','use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Economic stress: unemployment\n",
    "Social isolation: loneliness\n",
    "Domestic stress: domestic violence\n",
    "School lockdown: online learning\n",
    "Substance use: addiction\n",
    "Suicide: suicide ideation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unemployment',\n",
       " 'income',\n",
       " 'rent',\n",
       " 'jobless',\n",
       " 'credit',\n",
       " 'employment',\n",
       " 'companies',\n",
       " 'fortunately',\n",
       " 'laid',\n",
       " 'apply',\n",
       " 'positions',\n",
       " 'dollars',\n",
       " 'contract',\n",
       " 'employees',\n",
       " 'parttime',\n",
       " 'salary',\n",
       " 'staff',\n",
       " 'report',\n",
       " 'paycheck',\n",
       " 'programs',\n",
       " 'graduating',\n",
       " 'luckily',\n",
       " 'pharmacy',\n",
       " 'parking',\n",
       " 'bills',\n",
       " 'pays',\n",
       " 'california',\n",
       " 'strength',\n",
       " 'homeless',\n",
       " 'encouraged',\n",
       " 'tasks',\n",
       " 'patients',\n",
       " 'promises',\n",
       " 'traumatized',\n",
       " 'hometown',\n",
       " 'wage',\n",
       " 'adjust',\n",
       " 'car',\n",
       " 'fulltime',\n",
       " 'circumstances',\n",
       " 'tech',\n",
       " 'pocket',\n",
       " 'demanding',\n",
       " 'repair',\n",
       " 'surprised',\n",
       " 'temporarily',\n",
       " 'courses',\n",
       " 'transition',\n",
       " 'hired',\n",
       " 'admitting']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('unemployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loneliness',\n",
       " 'boredom',\n",
       " 'enemy',\n",
       " 'insecurities',\n",
       " 'laziness',\n",
       " 'emptiness',\n",
       " 'abandonment',\n",
       " 'hopelessness',\n",
       " 'associated',\n",
       " 'waves',\n",
       " 'abyss',\n",
       " 'rot',\n",
       " 'sadness',\n",
       " 'darkness',\n",
       " 'liver',\n",
       " 'sorrow',\n",
       " 'romantic',\n",
       " 'passions',\n",
       " 'quietly',\n",
       " 'pushes',\n",
       " 'tolerance',\n",
       " 'worsening',\n",
       " 'vibes',\n",
       " 'powerless',\n",
       " 'bouts',\n",
       " 'urges',\n",
       " 'tiredness',\n",
       " 'factor',\n",
       " 'immense',\n",
       " 'ego',\n",
       " 'prone',\n",
       " 'utter',\n",
       " 'lips',\n",
       " 'selfesteem',\n",
       " 'loner',\n",
       " 'design',\n",
       " 'promises',\n",
       " 'realistic',\n",
       " 'moods',\n",
       " 'christian',\n",
       " 'porn',\n",
       " 'mixed',\n",
       " 'science',\n",
       " 'blade',\n",
       " 'misery',\n",
       " 'creates',\n",
       " 'creative',\n",
       " 'loathing',\n",
       " 'procrastinate',\n",
       " 'suffocating']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('loneliness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abuse',\n",
       " 'assault',\n",
       " 'verbal',\n",
       " 'abusing',\n",
       " 'christian',\n",
       " 'narcissistic',\n",
       " 'boys',\n",
       " 'sexual',\n",
       " 'violence',\n",
       " 'ties',\n",
       " 'poverty',\n",
       " 'addict',\n",
       " 'substance',\n",
       " 'traumas',\n",
       " 'traumatized',\n",
       " 'longterm',\n",
       " 'abused',\n",
       " 'stems',\n",
       " 'culture',\n",
       " 'pregnancy',\n",
       " 'serotonin',\n",
       " 'aka',\n",
       " 'divorced',\n",
       " 'verbally',\n",
       " 'sexually',\n",
       " 'foster',\n",
       " 'unwanted',\n",
       " 'neglect',\n",
       " 'abusive',\n",
       " 'que',\n",
       " 'childhood',\n",
       " 'rape',\n",
       " 'raped',\n",
       " 'trauma',\n",
       " 'bullying',\n",
       " 'household',\n",
       " 'intimacy',\n",
       " 'manipulative',\n",
       " 'deserved',\n",
       " 'committed',\n",
       " 'strict',\n",
       " 'addition',\n",
       " 'dependent',\n",
       " 'dysfunctional',\n",
       " 'alcoholic',\n",
       " 'zombie',\n",
       " 'addictive',\n",
       " 'aggressive',\n",
       " 'adulthood',\n",
       " 'ativan']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('abuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance',\n",
       " 'use',\n",
       " 'green',\n",
       " 'recommendations',\n",
       " 'magnesium',\n",
       " 'challenges',\n",
       " 'smoking',\n",
       " 'ketamine',\n",
       " 'savings',\n",
       " 'abuse',\n",
       " 'tea',\n",
       " 'programs',\n",
       " 'characters',\n",
       " 'marijuana',\n",
       " 'dismissed',\n",
       " 'sheer',\n",
       " 'occasional',\n",
       " 'warning',\n",
       " 'methods',\n",
       " 'abilify',\n",
       " 'mindfulness',\n",
       " 'thesis',\n",
       " 'kiss',\n",
       " 'rehab',\n",
       " 'neglect',\n",
       " 'illegal',\n",
       " 'army',\n",
       " 'tool',\n",
       " 'poverty',\n",
       " 'heat',\n",
       " 'chemicals',\n",
       " 'aka',\n",
       " 'alcohol',\n",
       " 'addition',\n",
       " 'foster',\n",
       " 'vitamins',\n",
       " 'nicotine',\n",
       " 'mechanism',\n",
       " 'script',\n",
       " 'porn',\n",
       " 'useful',\n",
       " 'acceptance',\n",
       " 'sends',\n",
       " 'reflection',\n",
       " 'longterm',\n",
       " 'compassion',\n",
       " 'healthier',\n",
       " 'structure',\n",
       " 'label',\n",
       " 'relieve']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words_2('substance','use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ideation',\n",
       " 'suicide',\n",
       " 'committing',\n",
       " 'tendencies',\n",
       " 'mention',\n",
       " 'pot',\n",
       " 'ward',\n",
       " 'immense',\n",
       " 'visits',\n",
       " 'justify',\n",
       " 'harm',\n",
       " 'report',\n",
       " 'attempted',\n",
       " 'intentions',\n",
       " 'facility',\n",
       " 'bullet',\n",
       " 'tapering',\n",
       " 'expenses',\n",
       " 'hospitalized',\n",
       " 'reduced',\n",
       " 'acted',\n",
       " 'receiving',\n",
       " 'range',\n",
       " 'mentioning',\n",
       " 'holds',\n",
       " 'gods',\n",
       " 'commitment',\n",
       " 'scenario',\n",
       " 'ongoing',\n",
       " 'attempting',\n",
       " 'suicidal',\n",
       " 'quitting',\n",
       " 'ending',\n",
       " 'programs',\n",
       " 'persistent',\n",
       " 'violence',\n",
       " 'selfharm',\n",
       " 'spoke',\n",
       " 'decades',\n",
       " 'joking',\n",
       " 'warning',\n",
       " 'boredom',\n",
       " 'jumping',\n",
       " 'unnecessary',\n",
       " 'fearful',\n",
       " 'rehab',\n",
       " 'hospital',\n",
       " 'boundaries',\n",
       " 'jobless',\n",
       " 'downhill']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words_2('suicide','ideation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infection',\n",
       " 'ekg',\n",
       " 'tumor',\n",
       " 'lung',\n",
       " 'grandfather',\n",
       " 'tooth',\n",
       " 'fever',\n",
       " 'mri',\n",
       " 'cough',\n",
       " 'sore',\n",
       " 'liver',\n",
       " 'tall',\n",
       " 'initial',\n",
       " 'finger',\n",
       " 'hill',\n",
       " 'repair',\n",
       " 'lungs',\n",
       " 'metal',\n",
       " 'injury',\n",
       " 'ringing',\n",
       " 'que',\n",
       " 'arm',\n",
       " 'foggy',\n",
       " 'cancer',\n",
       " 'allergic',\n",
       " 'tongue',\n",
       " 'sharp',\n",
       " 'diarrhea',\n",
       " 'ear',\n",
       " 'cell',\n",
       " 'parttime',\n",
       " 'damaged',\n",
       " 'seizures',\n",
       " 'notices',\n",
       " 'divorce',\n",
       " 'accident',\n",
       " 'grabbed',\n",
       " 'trembling',\n",
       " 'neck',\n",
       " 'vet',\n",
       " 'zaps',\n",
       " 'legs',\n",
       " 'loan',\n",
       " 'muscle',\n",
       " 'jaw',\n",
       " 'incident',\n",
       " 'ice',\n",
       " 'surgery',\n",
       " 'wore',\n",
       " 'thyroid']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('infection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CLOSEsimilar_words('recession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
